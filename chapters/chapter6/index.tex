\chapter{The System Evaluation}

\section{Scalability of Neighbour-Joining Workflow}

Scalability is the ability of a system to sustain increasing workloads by making use of additional resources. In the context of our clustering workflows, the workload is produced by data processing tasks that load the data into RDDs and perform transformations on RDDs. A data processing task is broken up across stages, therefore generates new RDDs and thus new partitions as the result of each stage. Each task is assigned to a single or multiple cores. The number of cores affact the level of parallism by driving the number of Spark executors required for a task. 

Each Spark executor hosts a single or mutliple data partitions that the tasks are executed. Thus the number of exectuors for a stage is driven by the factors of (1) the number of tasks of a stage, which is driven by the transformations within a workflow; (2) \textsf{spark.task.cpus}, the number of cores allocated for each task, which is one by default; and (3) \textsf{spark.executor.cores}, the number of cores to use on each executor, which is one by default. 


In our evalaution experiments, we remain the same dataset and vary total number of cores to observe the processing time of the same workload. Figure~\ref{Fig:scalability-nj} plots the processing time of the Neighbour-Joining workflow as the number of cores scales from 1 to 55. Beyond 55 cores, it has not been observed any improvement. The processing time is more than 8 minutes when only a single core is set. The optimal processing time is reduced to approximately 3 minutes at 12 cores allocated. Over 12 cores, the overhead of data partition on distributed cores surpasses the performance gain of parallesim. 


\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/workflow1performance}
	\caption{Scalability of the Neigbhour-Joining workflow}
	\label{Fig:scalability-nj}
	
\end{figure}

To further identify the tasks that contribute most to the processing time, we decompose the processing time as ploted in Figure~\ref{Fig:nj-decomposition} when the number of cores changes from 10 to 20. The plot shows the most significant change is the percentage of \textsf{Shuffle Read/Write Time} that increases from 7.6\% to 17.18\%, more than twice.  This confirms that increasing the number of cores incurs the higher level of parallelism but more overhead of data shuffling. 


\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=2.8in, width=2.8in]{./figures/nj-piechart-10cores}
	\includegraphics[keepaspectratio,height=2.8in, width=2.8in]{./figures/nj-piechart-20cores}
	\caption{Processing time decomposition of Neighour-Joining tasks by varying number of cores}
	\label{Fig:nj-decomposition}
\end{figure}

\section{Scalability of DBSCAN Workflow}
The DBSCAN workflow is evaluated in the same experiment setup. The processing time of the workflow is measured by varying the number of cores. The measured processing time includes including the stages of DTW and PCA.  The plot in Figure~\ref{Fig:scalability-dbscan} shows 
the processing time of the workflow is approximately of 6 minutes. After the system reaches 20 cores, the processing time remains a plateau value around 1 minute even when the number of cores increases from 20 to 60 cores. The observation is different to the scalability plot of the Neighbour-Joining workflow. The plot shows changing the number of cores that increases the level of parallesim has no significant effect on the processing time.  

\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/workflow2performance}
	\caption{Scalability of the DBSCAN workflow}
	\label{Fig:scalability-dbscan}
\end{figure}


To identify the intrisic factors leading to this scalability behavior, we decompose the processing time to tasks as shown in the top chart of Figure~\ref{Fig:dbscan-decomposition}. It shows the majority of the processing time is spent on data preprocessing and generation of distance metrics as inputs to PCA and DBSCAN. 

We further compare the distribution of the processing time given different distance metrics. We compare the distance metrics generated by fast DTW and by correlation as used in the Neigbhour-Joining workflow. Figure~\ref{Fig:dbscan-decomposition} shows the fast DTW occupies approximately 58\% of the total process time, while the correlation method only incurs 15\% of the total processing time. 

Above results indicate the operations in the DTW-PCA-DBSCAN workflow reach to the optimal level of parallelsim running on 20 cores given the dataset. The data shuffling load in this workflow is not a dominating factor and thus increasing the number of cores does not adversarially cause overhead on the processing time. 


\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/dbscan-dtw}
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/dbscan-correlation}
	\caption{Processing time decomposition of DBSCAN tasks with different distance metrics}
	\label{Fig:dbscan-decomposition}
\end{figure}


\section{Parallism and Performance Tuning}

%The performance of both workflows are affected by common factors related to research questions we presented in the %Introduction section, namely \circled{R1} data correlations; \circled{R2} in-memory representation of high dimention and %partitioned data sets; and \circled{R3} algorithms to transform the data. In this section, we discuss the techniques of %performance tuning for each factor we have identified within the workflows. 

The performance of both workflows are affected by common factors related to research questions presented in the introduction section, namely \textbf{(R1)} data correlation; \textbf{(R2)} in-memory representation of high dimention and partitioned data sets; and \textbf{(R3)} algorithms to transform the data. In this section, we discuss the techniques of performance tuning for each factor we have identified within the workflows. 


\subsection{Reduce Data Shuffling}
Data shuffling is caused when tasks cross multiple stages need the same  data, the data on demand is transferred across the network and passes through the software stack. An example of these operations are \textsf{ReduceByKey}, \textsf{GroupByKey} from Spark. In these operations, all the key value tuples from all partitions are shuffled across the cluster to conform to the reducing rule. This generates significant data transferred over the network and negatively affects the end-to-end performance of the analytic workflow.

The optimization technique is using Spark broadcast variables to keep read-only data  cached on each node. The preprocessed time series data (see Figure~\ref{Fig:binning-10} and Figure~\ref{Fig:binning-20}) are stored in broadcast variables on each worker nodes. The broadcast variables are also used in the Neighbour-Joining method (see Algorithm~\ref{alg:pnj}). 

These broadcast variables are stored and retrieved from \textsf{hashmap} by the correspondig keys. We observe that the byte size of \textsf{Shuffle Read} reduces to zero during those tasks when the broadcast variables are applied as plotted in Figure~\ref{fig:broadcastreduceshuffling}.

\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/broadcastreduceshuffling}
	\caption{ Effect of a Spark transformation using Broadcast on Job Shuffle Read  }
	\label{fig:broadcastreduceshuffling}
\end{figure}



\subsection{Prune Matrix Operations}
Pruning matrix operations further optimizes the performance. The pairwise matrix in Neighbour-Joining and DTW is diagnoally symmetric that means the upper diagonal contains the same information as the lower diagonal. Therefore, we prune the computation on lower matrix by mappping the computation from the upper matrix to the lower marix.  

\subsection{Truncate RDD Lineage Graph}
A RDD lineage is a graph of all the parent RDDs of a RDD. The lineage graph is built when certain traformations are executed on the RDD. Meanwhile, a logical execution plan involving the parent RDD is created. This lineage graph also serves the base of resiliency as it allows to recompute missing or damaged partitions due to node failures. One issue we have encountered is the lineage graph is repeatively re-processing transformations within all previous iterations in every following iteration of the Neighbour-Joining algorithm. This is oberved as the phenomenon that each iteration takes longer delay then its previous one. Our solution is to truncate the RDD lineage graph at the end of each iteration, using the method of \textsf{RDD.checkpoint()} to save the previous computing result in RDDs to a HDFS filesystem. 

\subsection{Coalesce and Repartition}
The intermeidate transformation generates extra data even larger than the original input data size. An example is the \textsf{RDD.union()} transformation used in each iteration of the Neighor-Joining algorithm to combine RDDs that causes shuffling. If similar keys or range of keys are stored in the same partition then the shuffling is minimized and the processing of \textsf{union()} becomes substantially fast. However, we have observed that the number of partitions keep doubling each time the \textsf{RDD.union()} transformation is invoked. This causes unnecessary data shuffling from/to new partitions. To solve this problem, we explicitly repartition the RDDs. There are two partition methods:

\begin{enumerate}
	\item \textsf{Repartition} allows increasing or decreasing of the number of partitions; 
	\item \textsf{Coleasce} only decreases the number of partitions.
\end{enumerate} 

The selection of the repartition method considers two factors: (1) the processing time of the algorithm; (2) when and how often the repartition should be applied. 

\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/coalesce}
	\caption{Comparing Coalesce and Repartition Effect for Processing Time}
	\label{fig:repartition}
	
\end{figure}

For the evaluation purpose, we compare both methods by measuring the processing time by changing the number of iterations before one partition method is invoked. The plot in Figure~\ref{fig:repartition} shows that colease performs up to $8.5\%$ better than repartition when it is applied at every iteration. The plot also indicates that repartition at every iteration in the case of Neigbhour-Joining algoirthm on our dataset delivers better performance results.   


\subsection{Reduce Time Series Binning}

We have applied the time series binning techniques in the data preprocessing and alginment (see Section~\ref{sec:dataprocessing}). In a nutshell, the binning technique consists of reducing a window of values into a single value. The bigger the window, the less information kept on the actual time series. The plots in Figure~\ref{Fig:levelofbinning} displays different level of binnings with window size of 10, 20, 40 and 60. We observe as the window size increases, less outlining points appear in the PCA projection.  The binning window size becomes a tunning parameter as the trade-off between processing time and information retention. 

\begin{figure}
	\centering
	\includegraphics[keepaspectratio,height=3in, width=3in]{./figures/resampleeffecet}
	\caption{ Time series in PCA projection with different levels of binning}\label{Fig:levelofbinning}
\end{figure}
